{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "## Review\n",
    "\n",
    "We built up to an agent with memory:\n",
    "\n",
    "**act** - let the model call specific tools\n",
    "**observe** - pass the tool output back to the model\n",
    "**reason** - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
    "**persist state** - use an in memory checkpointer to support long-running conversations with interruptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "Now, we'll cover how to actually deploy our agent locally to Studio and to LangGraph Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the URL of your own deployed graph\n",
    "URL = \"http://localhost:52563\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       "  'graph_id': 'agent',\n",
       "  'created_at': '2024-11-24T11:50:25.808584+00:00',\n",
       "  'updated_at': '2024-11-24T11:50:25.808584+00:00',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'version': 1,\n",
       "  'name': 'agent'},\n",
       " {'assistant_id': '228f9934-0cdd-5383-92c8-ee8422522cc2',\n",
       "  'graph_id': 'router',\n",
       "  'created_at': '2024-11-23T18:07:20.528540+00:00',\n",
       "  'updated_at': '2024-11-23T18:07:20.528540+00:00',\n",
       "  'config': {},\n",
       "  'metadata': {'created_by': 'system'},\n",
       "  'version': 1,\n",
       "  'name': 'router'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a thread for tracking the state of our run\n",
    "thread = await client.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run our agent with client.runs.stream with:\n",
    "\n",
    "The **thread_id** \\\n",
    "The **graph_id** \\\n",
    "The **input** \\\n",
    "The **stream_mode** \\\n",
    "We'll discuss streaming in depth in a future module.\n",
    "\n",
    "For now, just recognize that we are streaming the full value of the state after each step of the graph with stream_mode=\"values\".\n",
    "\n",
    "The state is captured in the chunk.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'f99c5584-1210-4473-b15e-1e9868c00a15', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'id': 'call_m8dqeQGGZGdrIYR6BY86gn6x', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 135, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-e291ce47-7364-4c6b-b7ea-82a27740b409-0', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 2}, 'id': 'call_m8dqeQGGZGdrIYR6BY86gn6x', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 135, 'output_tokens': 17, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '6605f25b-df2f-436c-a199-3fe890fa4291', 'tool_call_id': 'call_m8dqeQGGZGdrIYR6BY86gn6x', 'artifact': None, 'status': 'success'}\n",
      "{'content': 'The result of multiplying 3 by 2 is 6.', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 14, 'prompt_tokens': 160, 'total_tokens': 174, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'run-bdf8ed75-cdc5-4bed-adb7-39d90651c648-0', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 160, 'output_tokens': 14, 'total_tokens': 174, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Input\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
    "\n",
    "# Stream\n",
    "async for chunk in client.runs.stream(\n",
    "        thread['thread_id'],\n",
    "        \"agent\",\n",
    "        input=input,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "    if chunk.data and chunk.event != \"metadata\":\n",
    "        print(chunk.data['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
